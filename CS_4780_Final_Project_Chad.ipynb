{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CS 4780 Final Project Chad.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jria8E5v1CMf"
      },
      "source": [
        "<h2>CS 4780/5780 Final Project: </h2>\n",
        "<h3>Election Result Prediction for US Counties</h3>\n",
        "\n",
        "Names and NetIDs for your group members:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3H93FI5wNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d818cd-5edb-447c-cb5d-488070a461cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCA3PgcU1CMh"
      },
      "source": [
        "<h3>Introduction:</h3>\n",
        "\n",
        "<p> The final project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The programming project provide templates for how to do this, and the most recent video lectures summarize some of the tricks you will need (e.g. feature normalization, feature construction). So, this final project brings realism to how you will use machine learning in the real world.  </p>\n",
        "\n",
        "The task you will work on is forecasting election results. Economic and sociological factors have been widely used when making predictions on the voting results of US elections. Economic and sociological factors vary a lot among counties in the United States. In addition, as you may observe from the election map of recent elections, neighbor counties show similar patterns in terms of the voting results. In this project you will bring the power of machine learning to make predictions for the county-level election results using Economic and sociological factors and the geographic structure of US counties. </p>\n",
        "<p>\n",
        "\n",
        "<h3>Your Task:</h3>\n",
        "Plase read the project description PDF file carefully and make sure you write your code and answers to all the questions in this Jupyter Notebook. Your answers to the questions are a large portion of your grade for this final project. Please import the packages in this notebook and cite any references you used as mentioned in the project description. You need to print this entire Jupyter Notebook as a PDF file and submit to Gradescope and also submit the ipynb runnable version to Canvas for us to run.\n",
        "\n",
        "<h3>Due Date:</h3>\n",
        "The final project dataset and template jupyter notebook will be due on <strong>December 15th</strong> . Note that <strong>no late submissions will be accepted</strong>  and you cannot use any of your unused slip days before.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNOZYvpj1CMi"
      },
      "source": [
        "![image.png; width=\"100\";](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFgyMZsY1CMj"
      },
      "source": [
        "<h2>Part 1: Basics</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5MgzyMQ1CMj"
      },
      "source": [
        "<h3>1.1 Import:</h3><p>\n",
        "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
        "    \n",
        "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
        "    \n",
        "https://pytorch.org/tutorials/\n",
        "    \n",
        "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atjgk4zl1CMm"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import sklearn.tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import  f1_score\n",
        "\n",
        "\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQTDNOYN1JHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54987ef2-b293-41e6-c832-0b7d471e997f"
      },
      "source": [
        "!kaggle competitions download -c cs-4780-final-project-county-prediction-basic\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTMhwyk21CMm"
      },
      "source": [
        "<h3>1.2 Weighted Accuracy:</h3><p>\n",
        "Since our dataset labels are heavily biased, you need to use the following function to compute weighted accuracy throughout your training and validation process and we use this for testing on Kaggle.\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrTVBa8K1CMn"
      },
      "source": [
        "def weighted_accuracy(pred, true):\n",
        "    assert(len(pred) == len(true))\n",
        "    num_labels = len(true)\n",
        "    num_pos = sum(true)\n",
        "    num_neg = num_labels - num_pos\n",
        "    frac_pos = num_pos/num_labels\n",
        "    weight_pos = 1/frac_pos\n",
        "    weight_neg = 1/(1-frac_pos)\n",
        "    num_pos_correct = 0\n",
        "    num_neg_correct = 0\n",
        "    for pred_i, true_i in zip(pred, true):\n",
        "        num_pos_correct += (pred_i == true_i and true_i == 1)\n",
        "        num_neg_correct += (pred_i == true_i and true_i == 0)\n",
        "    weighted_accuracy = ((weight_pos * num_pos_correct) \n",
        "                         + (weight_neg * num_neg_correct))/((weight_pos * num_pos) + (weight_neg * num_neg))\n",
        "    return weighted_accuracy"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQHfOqQ-1CMp"
      },
      "source": [
        "<h2>Part 2: Baseline Solution</h2><p>\n",
        "Note that your code should be commented well and in part 2.4 you can refer to your comments. (e.g. # Here is SVM, \n",
        "# Here is validation for SVM, etc). Also, we recommend that you do not to use 2012 dataset and the graph dataset to reach the baseline accuracy for 68% in this part, a basic solution with only 2016 dataset and reasonable model selection will be enough, it will be great if you explore thee graph and possibly 2012 dataset in Part 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP93YVn-1CMp"
      },
      "source": [
        "<h3>2.1 Preprocessing and Feature Extraction:</h3><p>\n",
        "Given the training dataset and graph information, you need to correctly preprocess the dataset (e.g. feature normalization). For baseline solution in this part, you might not need to introduce extra features to reach the baseline test accuracy.\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXpYgztG1CMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6121da-0be9-43fa-925e-f65aeb740ec7"
      },
      "source": [
        "# You may change this but we suggest loading data with the following code and you may need to change\n",
        "# datatypes and do necessary data transformation after loading the raw data to the dataframe.\n",
        "\n",
        "#Renamed columns\n",
        "\n",
        "train = pd.read_csv(\"drive/MyDrive/CS_4780_Project/train_2016.csv\", sep=',',header=0, encoding='unicode_escape')\n",
        "test=pd.read_csv(\"drive/MyDrive/CS_4780_Project/test_2016_no_label.csv\",sep=',',header=0, encoding='unicode_escape')\n",
        "#Determining which party won each county, 0= GOP win, 1- DEM win\n",
        "labels= train[\"DEM\"]>train[\"GOP\"]\n",
        "\n",
        "#label dataset\n",
        "labels=labels.astype(int)\n",
        "#using FIPS code as index\n",
        "train=train.set_index(\"FIPS\")\n",
        "test=test.set_index(\"FIPS\")\n",
        "#dropped county since it is not a numerical feature\n",
        "train=train.drop('County', axis=1)\n",
        "test=test.drop('County', axis=1)\n",
        "#------------------------\n",
        "train=train.drop('DEM', axis=1)\n",
        "train=train.drop('GOP', axis=1)\n",
        "\n",
        "#----------------\n",
        "\n",
        "#Converting income numbers to floats\n",
        "train[\"MedianIncome\"]=train[\"MedianIncome\"].str.replace(\",\",\"\")\n",
        "\n",
        "train[\"MedianIncome\"]=pd.to_numeric(train [\"MedianIncome\"], downcast=\"float\")\n",
        "\n",
        "test[\"MedianIncome\"]=test[\"MedianIncome\"].str.replace(\",\",\"\")\n",
        "\n",
        "test[\"MedianIncome\"]=pd.to_numeric(test[\"MedianIncome\"], downcast=\"float\")\n",
        "\n",
        "\n",
        "\n",
        "print(train)\n",
        "# Make sure you comment your code clearly and you may refer to these comments in the part 2.4\n",
        "# TODO"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       MedianIncome  MigraRate  ...  BachelorRate  UnemploymentRate\n",
            "FIPS                            ...                                \n",
            "18019       51837.0        4.9  ...          20.9               4.2\n",
            "6035        49793.0      -18.4  ...          12.0               6.9\n",
            "40081       44914.0       -1.3  ...          15.1               5.3\n",
            "31153       74374.0        9.2  ...          40.1               2.9\n",
            "28055       26957.0      -12.8  ...           6.7              14.0\n",
            "...             ...        ...  ...           ...               ...\n",
            "36009       46224.0       -3.5  ...          19.1               6.0\n",
            "55031       50340.0       -2.6  ...          24.0               5.2\n",
            "27065       51347.0        1.6  ...          14.6               6.5\n",
            "17139       57447.0       -9.2  ...          19.5               4.6\n",
            "20185       46104.0       -7.9  ...          23.9               3.7\n",
            "\n",
            "[1555 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjzjkQYFiKVP",
        "outputId": "794f91c3-7a7c-491e-dfc5-ac0d77e84fcf"
      },
      "source": [
        "#NN preprocessing\r\n",
        "\r\n",
        "\r\n",
        "nntrain=train.to_numpy()\r\n",
        "nnlabels=labels.to_numpy()\r\n",
        "nntest=test.to_numpy()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6DEtbR1CMq"
      },
      "source": [
        "<h3>2.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
        "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 1.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMwuH8Eb1CMq"
      },
      "source": [
        "#Using Decision Tree with information gain as the split criterion- this \n",
        "#ensures tree does not get too deep and thus prevents overfitting\n",
        "\n",
        "D_tree_A=sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", splitter= \"random\",)\n",
        "\n",
        "#Neural network\n",
        "#Using Multilayer perceptron neural network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d7qjxA-fac3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNy9HqJC1CMr"
      },
      "source": [
        "<h3>2.3 Training, Validation and Model Selection:</h3><p>\n",
        "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsRT5FCw1CMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93bea96-7f1d-4f3f-efa5-ff4d4ad6e231"
      },
      "source": [
        "\n",
        "#Split training data into training and validation sets\n",
        "\n",
        "x_train, x_val, y_train, y_val=sklearn.model_selection.train_test_split(train, labels, test_size=0.25)\n",
        "\n",
        "\n",
        "\n",
        "#Train Decision trees with different depths\n",
        "\n",
        "#average best depths of multiple groups of trees\n",
        "bestdepths=[]\n",
        "\n",
        "for x in range (1,10):\n",
        "  acc_dictionary={}\n",
        "#Test for best depth over a number of trees\n",
        "  for i in range(1,30):\n",
        "    D_tree=sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", splitter= \"random\",max_depth=i)\n",
        "    D_tree.fit(x_train,y_train)\n",
        "\n",
        "    # Measure accuracy each has on validation set using weighted accuracy function\n",
        "    val_preds=D_tree.predict(x_val)\n",
        "    acc= weighted_accuracy(val_preds,y_val )\n",
        "    acc=acc*100\n",
        "\n",
        "    # Store dictionary of accuracies and their depths\n",
        "    acc_dictionary[acc] = \"{}\".format(i)    \n",
        "    best= acc_dictionary[max(acc_dictionary)]    \n",
        "    bestdepths.append(int(best))\n",
        "\n",
        "\n",
        "#Choose average of best depths\n",
        "\n",
        "finaldp=sum(bestdepths)/len(bestdepths)\n",
        "\n",
        "\n",
        "#Find best number of leaf nodes\n",
        "\n",
        "acc_dictionary={}\n",
        "for i in [20,50,100,500,1000]:\n",
        "    D_tree=sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", splitter= \"random\",max_depth=finaldp, max_leaf_nodes=i)\n",
        "    D_tree.fit(x_train,y_train)\n",
        "    \n",
        "    \n",
        "    # Measure accuracy each has on validation set using weighted accuracy function\n",
        "    val_preds=D_tree.predict(x_val)\n",
        "    acc= weighted_accuracy(val_preds,y_val )\n",
        "    acc=acc*100\n",
        "    \n",
        "    \n",
        "\n",
        "    # Store dictionary of accuracies and their depths\n",
        "    acc_dictionary[acc] = i\n",
        "best= acc_dictionary[max(acc_dictionary)]\n",
        "final_max_nodes=best\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "50\n",
            "100\n",
            "500\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raDCKvOZKdx8",
        "outputId": "443d3d59-5a3e-4bb7-f79f-cef6d280ffdf"
      },
      "source": [
        "#Neural Network\r\n",
        "\r\n",
        "#Split training data into training and validation sets\r\n",
        "xnn_train, xnn_val, ynn_train, ynn_val=sklearn.model_selection.train_test_split(nntrain, labels, test_size=0.25)\r\n",
        "\r\n",
        "\r\n",
        "#Tune activiation function\r\n",
        "#Create a classifier with each activation function and test to see which function gives the best weighted accuracy\r\n",
        "acc_dictionary={}\r\n",
        "for func in ['identity', 'logistic', 'tanh', 'relu']:\r\n",
        "  net=sklearn.neural_network.MLPClassifier(activation=func)\r\n",
        "  net.fit(xnn_train,ynn_train)\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  valnn_preds=net.predict(xnn_val)\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "  acc= weighted_accuracy(valnn_preds,ynn_val)\r\n",
        "\r\n",
        "  \r\n",
        "  acc=acc*100\r\n",
        "\r\n",
        "  # Store dictionary of accuracies and the function that gave them\r\n",
        "  acc_dictionary[acc] = func\r\n",
        "        \r\n",
        "  best= acc_dictionary[max(acc_dictionary)] \r\n",
        "\r\n",
        "print(\"best func is\", best)\r\n",
        "    \r\n",
        "    \r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Tune hidden layer \r\n",
        "acc_dictionary={}\r\n",
        "for k in [10,25,50,75,100]:\r\n",
        "  net=sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(k,))\r\n",
        "  \r\n",
        "  net.fit(xnn_train, ynn_train)\r\n",
        "  valnn_preds=net.predict(xnn_val)\r\n",
        "  acc= weighted_accuracy(valnn_preds,ynn_val)\r\n",
        "  print (acc)\r\n",
        "  \r\n",
        "  \r\n",
        "\r\n",
        "  # Store dictionary of accuracies and the function that gave them\r\n",
        "  acc_dictionary[acc] = k\r\n",
        "        \r\n",
        "best= acc_dictionary[max(acc_dictionary)] \r\n",
        "print(\"best lay num is\", best)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Tune max iter\r\n",
        "\r\n",
        "acc_dictionary={}\r\n",
        "\r\n",
        "for m in [50,100,200,500,1000]:\r\n",
        "    net=sklearn.neural_network.MLPClassifier(hidden_layer_sizes=50 ,activation='relu', max_iter=m)\r\n",
        "    net.fit(xnn_train, ynn_train)\r\n",
        "    valnn_preds=net.predict(xnn_val)\r\n",
        "    acc= weighted_accuracy(valnn_preds,ynn_val)\r\n",
        "    \r\n",
        "\r\n",
        "    # Store dictionary of accuracies and the max_iter value that gave them\r\n",
        "    acc_dictionary[acc] = m\r\n",
        "          \r\n",
        "best= acc_dictionary[max(acc_dictionary)] \r\n",
        "\r\n",
        "print(\"best max_iter is\", m)\r\n",
        "\r\n",
        "#Tune alpha\r\n",
        "\r\n",
        "acc_dictionary={}\r\n",
        "\r\n",
        "for alpha in[0.0010, 0.0015,0.0020,0.0100,0.0200] :\r\n",
        "    net=sklearn.neural_network.MLPClassifier(hidden_layer_sizes=50 ,activation='relu', max_iter=1000, alpha=alpha)\r\n",
        "    net.fit(xnn_train, ynn_train)\r\n",
        "    valnn_preds=net.predict(xnn_val)\r\n",
        "    acc= weighted_accuracy(valnn_preds,ynn_val)\r\n",
        "    \r\n",
        "    # Store dictionary of accuracies and the learning rate that gave them\r\n",
        "    acc_dictionary[acc] = alpha\r\n",
        "          \r\n",
        "best= acc_dictionary[max(acc_dictionary)] \r\n",
        "\r\n",
        "print(\"best alpha is\", best)\r\n",
        "\r\n",
        "#Tuning learning rate init\r\n",
        "acc_dictionary={}\r\n",
        "\r\n",
        "for lrate in[0.0001, 0.0005,0.0010, 0.0015,0.0020] :\r\n",
        "    net=sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100) ,activation='relu', max_iter=1000, learning_rate_init=lrate)\r\n",
        "    net.fit(xnn_train, ynn_train)\r\n",
        "    valnn_preds=net.predict(xnn_val)\r\n",
        "    acc= weighted_accuracy(valnn_preds,ynn_val)\r\n",
        "       \r\n",
        "    # Store dictionary of accuracies and the learning rate that gave them\r\n",
        "    acc_dictionary[acc] = lrate\r\n",
        "          \r\n",
        "best= acc_dictionary[max(acc_dictionary)] \r\n",
        "\r\n",
        "print(\"best learning rate is\", best)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best func is relu\n",
            "0.5\n",
            "0.5\n",
            "0.4488744451490171\n",
            "0.49999999999999994\n",
            "0.5\n",
            "best lay num is 100\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "best max_iter is 1000\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "best alpha is 0.02\n",
            "0.5\n",
            "0.49999999999999994\n",
            "0.5\n",
            "0.49999999999999994\n",
            "0.5\n",
            "best learning rate is 0.002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZB9Jlck7lUg"
      },
      "source": [
        "#train and test best neural netowrk\r\n",
        "\r\n",
        "net=sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100) ,activation='relu', max_iter=1000, alpha=0.01, learning_rate_init=0.002)\r\n",
        "net.fit(xnn_train, ynn_train)\r\n",
        "nnpreds=net.predict(nntest)\r\n",
        "\r\n",
        "nnsub= pd.DataFrame({\"FIPS\": test.index ,\r\n",
        "             \"Result\": nnpreds \r\n",
        "             })\r\n",
        "nnsub=nnsub.set_index(\"FIPS\")\r\n",
        "nnsub.to_csv( \"drive/MyDrive/CS_4780_Project/nnsoln.csv\")\r\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1jRBpqjaR5V",
        "outputId": "eb67f52e-7961-4f93-e03c-509868ad240a"
      },
      "source": [
        "#train and test best tree\r\n",
        "\r\n",
        "D_tree_final=sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", splitter= \"random\",\r\n",
        "                                                 max_depth=finaldp,max_leaf_nodes= final_max_nodes )\r\n",
        "D_tree_final.fit(x_train,y_train)\r\n",
        "preds=D_tree_final.predict(x_val)\r\n",
        "val_accy=weighted_accuracy(preds,y_val)\r\n",
        "\r\n",
        "submissionpreds=D_tree_final.predict(test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "submission= pd.DataFrame({\"FIPS\": test.index ,\r\n",
        "             \"Result\": submissionpreds \r\n",
        "             })\r\n",
        "\r\n",
        "submission=submission.set_index(\"FIPS\")\r\n",
        "\r\n",
        "submission.to_csv( \"drive/MyDrive/CS_4780_Project/Basic_Soln1.2.csv\")\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9228295819935691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZPTYeIEiyL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWy9FyRB1CMr"
      },
      "source": [
        "<h3>2.4 Explanation in Words:</h3><p>\n",
        "    You need to answer the following questions in the markdown cell after this cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcEuJRr71CMr"
      },
      "source": [
        "2.4.1 How did you preprocess the dataset and features?\n",
        "\n",
        "The column with county names was removed, since the sklearn decision tree classifier can only take numerical data, and the FIPS codes already present a unique identifier for each county.\n",
        "\n",
        "The data in the other columns was converted from strings to floats in order for the sklearn classifiers to work well with them.\n",
        "\n",
        "2.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
        "\n",
        "I used a decision tree and neural network. A decision tree is convenient because it\n",
        "\n",
        "The neural network is a multilayer perceptron. This algorithm is capable of approxiamating any continuous function and so it is ideal for situations in which the ideal function mapping the features to the labels is a complex one.\n",
        "There are many features that have complex relationships when it comes to which party wins a n election and so an algorithm that is capable of representing any relationship works well for this problem.\n",
        "\n",
        "\n",
        "2.4.3 How did you do the model selection?\n",
        "\n",
        "I split the training data into a training and validation set, with 3/4 used for training and 1/4 used for validation.\n",
        "\n",
        "Decision Tree\n",
        "\n",
        "I tested the accuracy of decision trees with different maximum leaf node limits and maximum depth limits. Each model was trained with the training set and its accuracy tested on the validation set.\n",
        "\n",
        " Initially, I used a for loop to test the accuracy of decision trees with different depths, during this process, I found that I was given a different value for the best depth each time, and so I opted to use nested for loops to test each depth several times and take the average of all of the best depths.\n",
        "\n",
        "\n",
        " Neural Network\n",
        "\n",
        "For the neural network I used for loops to test a wide range of values for learning rate, alpha, maximum number of iterations, activation function and hidden layer size. Interestingly changes to these hyperparameters had very little effect on the prediction accuracy.\n",
        "\n",
        "Changing the number of hidden layers and size of each layer also had very little effect on predictive accuracy.\n",
        "\n",
        "\n",
        "2.4.4 Does the test performance reach a given baseline 68% performance? (Please include a screenshot of Kaggle Submission)\n",
        "\n",
        "Yes, the decision tree reached around 70% accuracy on Kaggle.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDTGNhutitEn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQVPa7X1CMs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVDdo57t1CMs"
      },
      "source": [
        "<h2>Part 3: Creative Solution</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO3HXWjc1CMt"
      },
      "source": [
        "<h3>3.1 Open-ended Code:</h3><p>\n",
        "You may follow the steps in part 2 again but making innovative changes like creating new features, using new training algorithms, etc. Make sure you explain everything clearly in part 3.2. Note that reaching the 75% creative baseline is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBVUJiCb1CMt"
      },
      "source": [
        "# Make sure you comment your code clearly and you may refer to these comments in the part 3.2\n",
        "\n",
        "#Random Forest\n",
        "#Use randomizezed cross validation to conduct k fold cross validation for a large number of different parameter combinations\n",
        "#List of possible valuse to be considered for each parameter. Each iteration of randomsearchcv will use a combination of values\n",
        "#from these lists and record the accuracy using weighted accuracy function\n",
        "\n",
        "grid = {'n_estimators': [int(q) for q in np.linspace(start=100, stop=1500, num=100)],\n",
        "               'max_features': ['auto'],\n",
        "               'max_depth': [int(q) for q in (range(10,100))],             #10,100\n",
        "               'bootstrap':[True, False]}\n",
        "\n",
        "#scorer=make_scorer(sklearn.metrics.f1_score)\n",
        "from sklearn.metrics import make_scorer\n",
        "scorer = make_scorer(weighted_accuracy, greater_is_better=True)\n",
        "searcher=sklearn.model_selection.RandomizedSearchCV(estimator = forest, param_distributions = grid, n_iter = 10, cv = 5, scoring=scorer )\n",
        "searcher.fit(nntrain,nnlabels)\n",
        "\n",
        "#give best parameters\n",
        "print(searcher.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOHwmeXfollO",
        "outputId": "c0d64f35-598b-4f4a-b7d8-c430703b46a9"
      },
      "source": [
        "forest= sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_features='auto', max_depth=18, bootstrap=True)\r\n",
        "forest.fit(xnn_train,ynn_train)\r\n",
        "frstpred=forest.predict(xnn_val)\r\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7072500528429507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x0rZki5fSN3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKKk4MlufSco"
      },
      "source": [
        "#Create submission\r\n",
        "creativepreds=forest.predict(test)\r\n",
        "\r\n",
        "creativesub= pd.DataFrame({\"FIPS\": test.index ,\r\n",
        "             \"Result\": creativepreds \r\n",
        "             })\r\n",
        "\r\n",
        "creativesub=creativesub.set_index(\"FIPS\")\r\n",
        "\r\n",
        "creativesub.to_csv(\"drive/MyDrive/CS_4780_Project/creativesol.csv\")\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewZtoBZC1CMt"
      },
      "source": [
        "<h3>3.2 Explanation in Words:</h3><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYZEYT4y1CMt"
      },
      "source": [
        "You need to answer the following questions in a markdown cell after this cell:\n",
        "\n",
        "\n",
        "\n",
        "3.2.1 How much did you manage to improve performance on the test set compared to part 2? Did you reach the 75% accuracy for the test in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
        "No\n",
        "\n",
        "![submission](/drive/MyDrive/CS_4780_Project/submit2.png)\n",
        "\n",
        "\n",
        "3.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this.\n",
        "\n",
        "I used a random forest instead of a decision tree for the creative solution. Random forests are more robust that single decision trees. Since a random forest uses many decision trees it limits overfitting, thus reducing generalization error. \n",
        "\n",
        "This time around, for model selection I used a random search grid with kfold cross-validation, instead of testing one parameter at a time using for loops as I did in the basic solution, I simultaneously tested different parameter combinations. This allows one to test more possible models, thus increasing the chance of finding a model that generalizes well.\n",
        "\n",
        "In deciding what ranges of parameter values to test, special attention was paid to ensuring that n_estimaors (number of trees in the forest) was sizable and that max_depth was not too large in order to increase generalizability.\n",
        "\n",
        "A random search grid was chosen over a grid search (which tests all possible parameter combinations) in order to give the algorithm a reasonable runtime. Gridsearch may have yielded more accurate parameters but the runtime would have been too long.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE4kXx8p1CMu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cghu7EE51CMu"
      },
      "source": [
        "<h2>Part 4: Kaggle Submission</h2><p>\n",
        "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The CSV shall contain TWO column named exactly \"FIPS\" and \"Result\" and 1555 total rows excluding the column names, \"FIPS\" column shall contain FIPS of counties with same order as in the test_2016_no_label.csv while \"Result\" column shall contain the 0 or 1 prdicaitons for corresponding columns. A sample predication file can be downloaded from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRPxwtRx1CMu"
      },
      "source": [
        "# TODO\n",
        "\n",
        "# You may use pandas to generate a dataframe with FIPS and your predictions first \n",
        "# and then use to_csv to generate a CSV file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvRA-981CMu"
      },
      "source": [
        "<h2>Part 5: Resources and Literature Used</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIxj_mlt1CMv"
      },
      "source": [
        ""
      ]
    }
  ]
}